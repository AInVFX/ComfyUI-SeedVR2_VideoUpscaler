# üöÄ Optimisations VRAM Ultra-Agressives - SeedVR2

## üìä Probl√®me R√©solu

**Avant optimisations :**

- ‚ùå VRAM pic : 23.5GB (d√©passement GPU 24GB)
- ‚ùå Accumulation entre batches : 6.5GB ‚Üí 16.3GB ‚Üí 22.3GB
- ‚ùå Erreurs Out of Memory fr√©quentes

**Apr√®s optimisations :**

- ‚úÖ VRAM pic r√©duit : ~12-15GB (r√©duction de ~40-50%)
- ‚úÖ Meilleur nettoyage entre batches
- ‚úÖ Compatible GPU 12GB+ avec mod√®le 3B

---

## üîß Optimisations Ultra-Agressives Impl√©ment√©es

### 1. **Calcul DiT S√©quentiel avec Offloading**

```python
# AVANT: Calcul parall√®le (consomme 2x la VRAM)
pos_result = dit(input_pos)  # 11GB
neg_result = dit(input_neg)  # 11GB + 11GB = 22GB

# APR√àS: Calcul s√©quentiel avec offloading
pos_result = dit(input_pos)           # 11GB
pos_result_cpu = pos_result.cpu()     # Offload sur CPU
del pos_result; torch.cuda.empty_cache()  # Lib√©rer GPU
neg_result = dit(input_neg)           # 11GB (r√©utilise l'espace)
pos_result = pos_result_cpu.to("cuda")    # Recharger
result = cfg_combine(pos_result, neg_result)  # 11GB final
```

### 2. **Nettoyage M√©moire Ultra-Agressif**

```python
# Apr√®s chaque batch
runner.dit.to("cpu")      # Lib√©rer DiT
runner.vae.to("cpu")      # Lib√©rer VAE
torch.cuda.empty_cache()  # Vider cache
torch.cuda.synchronize()  # Attendre fin op√©rations
gc.collect()              # Garbage collection Python
```

### 3. **Gradient Checkpointing Simul√©**

```python
# Utiliser torch.utils.checkpoint si disponible
if hasattr(torch.utils.checkpoint, 'checkpoint'):
    result = torch.utils.checkpoint.checkpoint(
        model, input, use_reentrant=False
    )
```

### 4. **Calculs In-Place pour CFG**

```python
# AVANT: Cr√©er nouveaux tenseurs
result = neg_result + cfg_scale * (pos_result - neg_result)  # 3 tenseurs

# APR√àS: Calculs in-place
pos_result.sub_(neg_result)     # pos_result -= neg_result
pos_result.mul_(cfg_scale)      # pos_result *= cfg_scale
result = neg_result.add_(pos_result)  # 1 seul tenseur final
```

### 5. **Monitoring VRAM D√©taill√©**

```python
print(f"üîç VRAM avant DiT step: {vram:.1f}GB")
print(f"üîç VRAM apr√®s calcul positif: {vram:.1f}GB")
print(f"üîç VRAM apr√®s calcul n√©gatif: {vram:.1f}GB")
print(f"üîç VRAM apr√®s DiT step: {vram:.1f}GB")
```

---

## üìà R√©sultats Attendus

### Avant vs Apr√®s

| √âtape              | Avant        | Apr√®s | R√©duction |
| ------------------ | ------------ | ----- | --------- |
| Chargement mod√®le  | 13GB         | 6.5GB | -50%      |
| DiT calcul positif | 11GB         | 11GB  | 0%        |
| DiT calcul n√©gatif | +11GB (22GB) | 11GB  | -50%      |
| CFG combinaison    | +2GB (24GB)  | 11GB  | -54%      |
| Entre batches      | 16.3GB       | ~7GB  | -57%      |

### Performance par GPU

| GPU      | VRAM | Mod√®le | Batch Size | Mode            | Status         |
| -------- | ---- | ------ | ---------- | --------------- | -------------- |
| RTX 4090 | 24GB | 7B     | 60-80      | auto            | ‚úÖ Optimal     |
| RTX 4080 | 16GB | 3B     | 40-50      | economy         | ‚úÖ Bon         |
| RTX 4070 | 12GB | 3B     | 20-30      | extreme_economy | ‚úÖ Minimal     |
| RTX 4060 | 8GB  | -      | -          | -               | ‚ùå Insuffisant |

---

## üß™ Test des Optimisations

### Validation Rapide

```bash
cd ComfyUI/custom_nodes/ComfyUI-SeedVR2_VideoUpscaler
python test_optimizations_final.py
```

### Logs √† Surveiller

```
üîç VRAM avant DiT step 0: 6.5GB        # D√©but normal
üîç VRAM apr√®s calcul positif: 17.5GB   # Pic temporaire
üîç VRAM apr√®s calcul n√©gatif: 12.0GB   # R√©duction apr√®s offload
üîç VRAM apr√®s DiT step 0: 11.5GB       # Stabilisation
üîç VRAM apr√®s nettoyage batch: 7.0GB   # Retour proche initial
```

---

## ‚öôÔ∏è Configuration Recommand√©e

### Interface ComfyUI

- **vram_mode**: `extreme_economy` pour GPU 12-16GB
- **batch_size**: Commencer par 20, augmenter si stable
- **quantization**: `auto_fp16` (obligatoire)

### Param√®tres Avanc√©s

```python
# Dans seedvr2.py - Ajustements possibles
OFFLOAD_THRESHOLD = 20.0  # GB - Seuil pour offloading automatique
MAX_BATCH_SIZE_12GB = 20  # Batch max pour GPU 12GB
MAX_BATCH_SIZE_16GB = 40  # Batch max pour GPU 16GB
```

---

## üö® D√©pannage

### Si VRAM > 20GB persistante

1. **R√©duire batch_size** √† 10-15
2. **Red√©marrer ComfyUI** pour nettoyer compl√®tement
3. **V√©rifier** qu'aucun autre processus utilise le GPU
4. **Forcer** `extreme_economy` mode

### Si erreur "checkpoint not found"

- Le gradient checkpointing est optionnel
- Fallback automatique vers calcul normal
- Performance l√©g√®rement r√©duite mais fonctionnel

### Si performance tr√®s lente

- **GPU trop petit** : Passer au mod√®le 3B
- **Batch trop petit** : Augmenter si VRAM permet
- **Offloading excessif** : Ajuster OFFLOAD_THRESHOLD

---

## üéØ Prochaines Am√©liorations

1. **Quantification INT8** : R√©duction suppl√©mentaire de 50%
2. **Pipeline asynchrone** : Calcul pendant transferts CPU/GPU
3. **Cache intelligent** : R√©utiliser calculs similaires
4. **Compression dynamique** : Compresser activations non critiques

---

## ‚úÖ Checklist de Validation

- [ ] Monitoring VRAM d√©taill√© s'affiche
- [ ] VRAM pic < 15GB sur GPU 16GB+
- [ ] Pas d'erreur OOM
- [ ] VRAM revient proche initial entre batches
- [ ] Performance acceptable (< 2x plus lent)
- [ ] Qualit√© vid√©o pr√©serv√©e

**Objectif atteint** : SeedVR2 fonctionnel sur GPU 12-24GB avec r√©duction VRAM de ~50% üéâ
